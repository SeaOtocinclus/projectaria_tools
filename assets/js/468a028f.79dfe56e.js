"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[4248],{65395:(e,t,r)=>{r.r(t),r.d(t,{assets:()=>c,contentTitle:()=>s,default:()=>p,frontMatter:()=>n,metadata:()=>a,toc:()=>d});var i=r(74848),o=r(28453);const n={sidebar_position:30,title:"Eye Tracking"},s="Project Aria Eye Tracking",a={id:"open_models/eye_tracking",title:"Eye Tracking",description:"Overview",source:"@site/docs/open_models/eye_tracking.mdx",sourceDirName:"open_models",slug:"/open_models/eye_tracking",permalink:"/projectaria_tools/docs/open_models/eye_tracking",draft:!1,unlisted:!1,editUrl:"https://github.com/facebookresearch/projectaria_tools/tree/main/website/docs/open_models/eye_tracking.mdx",tags:[],version:"current",sidebarPosition:30,frontMatter:{sidebar_position:30,title:"Eye Tracking"},sidebar:"tutorialSidebar",previous:{title:"Egocentric Voxel Lifting (EVL)",permalink:"/projectaria_tools/docs/open_models/evl"},next:{title:"SceneScript",permalink:"/projectaria_tools/docs/open_models/scenescript"}},c={},d=[{value:"Overview",id:"overview",level:2}];function l(e){const t={a:"a",code:"code",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",ul:"ul",...(0,o.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(t.header,{children:(0,i.jsx)(t.h1,{id:"project-aria-eye-tracking",children:"Project Aria Eye Tracking"})}),"\n",(0,i.jsx)(t.h2,{id:"overview",children:"Overview"}),"\n",(0,i.jsxs)(t.p,{children:["Project Aria Eye Tracking provides an open source inference model and tooling that can be used to estimate eye gaze direction based on Aria device Eye Tracking camera images. This Pytorch model, trained on data from over ~1200 individuals and ~2M frames, produces outputs that use the ",(0,i.jsx)(t.a,{href:"/projectaria_tools/docs/data_formats/mps/mps_eye_gaze",children:"Pre March 2024 Eye Gaze Model"})," data structure."]}),"\n",(0,i.jsxs)(t.p,{children:["Use ",(0,i.jsx)(t.code,{children:"projectaria_eyetracking"})," to generate and visualize these outputs on downloaded or streaming data. These outputs will also be compatible with Project Aria Tools MPS eye gaze output and visualizers."]}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.a,{href:"https://github.com/facebookresearch/projectaria_eyetracking",children:"Project Aria Eye Tracking GitHub repository"}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsxs)(t.li,{children:["Download and run ",(0,i.jsx)(t.code,{children:"projectaria_eyetracking"})]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.a,{href:"/projectaria_tools/docs/data_formats/mps/mps_eye_gaze",children:"Pre March 2024 Eye Gaze Model"}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsx)(t.li,{children:"How the data is structured"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.a,{href:"/projectaria_tools/docs/data_utilities/core_code_snippets/eye_gaze_code",children:"Project Aria Tools Eye Gaze Code Snippets"}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsx)(t.li,{children:"Project Aria Tooling is compatible with these Eye Gaze outputs"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.a,{href:"/projectaria_tools/docs/ARK/sdk/setup",children:"Project Aria Client SDK"}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsx)(t.li,{children:"Code snippets that support streaming live Aria data to downstream applications"}),"\n"]}),"\n"]}),"\n"]})]})}function p(e={}){const{wrapper:t}={...(0,o.R)(),...e.components};return t?(0,i.jsx)(t,{...e,children:(0,i.jsx)(l,{...e})}):l(e)}},28453:(e,t,r)=>{r.d(t,{R:()=>s,x:()=>a});var i=r(96540);const o={},n=i.createContext(o);function s(e){const t=i.useContext(n);return i.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function a(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),i.createElement(n.Provider,{value:t},e.children)}}}]);