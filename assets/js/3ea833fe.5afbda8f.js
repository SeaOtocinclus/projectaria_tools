"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[4697],{88298:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>d,contentTitle:()=>l,default:()=>h,frontMatter:()=>s,metadata:()=>c,toc:()=>p});var n=a(74848),r=a(28453),o=a(49489),i=a(7227);const s={sidebar_position:45,title:"MPS - Eye Gaze"},l="Eye Gaze Code Snippets",c={id:"data_utilities/core_code_snippets/eye_gaze_code",title:"MPS - Eye Gaze",description:"This page provides a number of Eye Gaze code snippets that can be useful when working with Eye Gaze data provided in Open Datasets or generated by Machine Perception Services.",source:"@site/docs/data_utilities/core_code_snippets/eye_gaze_code.mdx",sourceDirName:"data_utilities/core_code_snippets",slug:"/data_utilities/core_code_snippets/eye_gaze_code",permalink:"/projectaria_tools/docs/data_utilities/core_code_snippets/eye_gaze_code",draft:!1,unlisted:!1,editUrl:"https://github.com/facebookresearch/projectaria_tools/tree/main/website/docs/data_utilities/core_code_snippets/eye_gaze_code.mdx",tags:[],version:"current",sidebarPosition:45,frontMatter:{sidebar_position:45,title:"MPS - Eye Gaze"},sidebar:"tutorialSidebar",previous:{title:"MPS - General",permalink:"/projectaria_tools/docs/data_utilities/core_code_snippets/mps"},next:{title:"Plot Sensor Data (Python)",permalink:"/projectaria_tools/docs/data_utilities/advanced_code_snippets/plotting_sensor_data"}},d={},p=[{value:"Further resources",id:"further-resources",level:3},{value:"General",id:"general",level:2},{value:"Load Eye Gaze in Python and C++",id:"load-eye-gaze-in-python-and-c",level:3},{value:"Yaw/Pitch to 3D vector conversion\u200b",id:"yawpitch-to-3d-vector-conversion",level:3},{value:"Get Central Pupil Frame(CPF) to Device Transforms (Python)",id:"get-central-pupil-framecpf-to-device-transforms-python",level:3},{value:"Helper functions to support the new Eye Gaze model",id:"helper-functions-to-support-the-new-eye-gaze-model",level:2},{value:"Get combined gaze direction and depth from left and right gaze direction angles",id:"get-combined-gaze-direction-and-depth-from-left-and-right-gaze-direction-angles",level:3},{value:"Get gaze intersection in 3D coordinates in CPF frame from left and right gaze direction angles",id:"get-gaze-intersection-in-3d-coordinates-in-cpf-frame-from-left-and-right-gaze-direction-angles",level:3},{value:"Get left and right gaze directions in CPF frame",id:"get-left-and-right-gaze-directions-in-cpf-frame",level:3}];function u(e){const t={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",ul:"ul",...(0,r.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(t.header,{children:(0,n.jsx)(t.h1,{id:"eye-gaze-code-snippets",children:"Eye Gaze Code Snippets"})}),"\n",(0,n.jsx)(t.p,{children:"This page provides a number of Eye Gaze code snippets that can be useful when working with Eye Gaze data provided in Open Datasets or generated by Machine Perception Services."}),"\n",(0,n.jsx)(t.h3,{id:"further-resources",children:"Further resources"}),"\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsx)(t.li,{children:(0,n.jsx)(t.a,{href:"/projectaria_tools/docs/data_formats/mps/mps_eye_gaze",children:"Eye Gaze Data Format"})}),"\n"]}),"\n",(0,n.jsx)(t.h2,{id:"general",children:"General"}),"\n",(0,n.jsx)(t.h3,{id:"load-eye-gaze-in-python-and-c",children:"Load Eye Gaze in Python and C++"}),"\n",(0,n.jsxs)(t.p,{children:["Data loaders for MPS outputs are provided as part of Project Aria Tools (",(0,n.jsx)(t.a,{href:"https://github.com/facebookresearch/projectaria_tools/blob/main/core/mps",children:"projectaria_tools/main/core/mps"}),"). As part of this, the loaders put the outputs into data structures that are easier for other tools to consume."]}),"\n",(0,n.jsx)(t.p,{children:"In this example, we set the default eye gaze depth to 1 meter. We use 1m because it is close to what people could reach with their arms and look at if they were grabbing objects. You can also use .depth to define depth_m when using data generated using the new eye gaze model."}),"\n",(0,n.jsxs)(o.default,{groupId:"programming-language",children:[(0,n.jsx)(i.default,{value:"python",label:"Python",children:(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-python",children:'import projectaria_tools.core.mps as mps\n\ngaze_path = "/path/to/mps/output/eye_gaze/general_eye_gaze.csv"\ngaze_cpf = mps.read_eyegaze(eye_gaze_path)\n\n# Set default eye gaze depth for 3D points to 1 meter\ndepth_m = 1.0\ngaze_point_cpf = mps.get_eyegaze_point_at_depth(gaze_cpf[1].yaw, gaze_cpf[1].pitch, depth_m)\n\n# Example query: find the nearest eye gaze data outputs in relation to a specific timestamp\nfrom projectaria_tools.core import data_provider\nfrom projectaria_tools.core.stream_id import StreamId\nfrom projectaria_tools.core.mps.utils import (\n    get_gaze_vector_reprojection,\n    get_nearest_eye_gaze\n)\n\n# Query Eye Gaze data at a desired timestamp\n# For this example we use an eyegaze data timestamp\n# You can also use a VRS timestamp (i.e timestamp from a loop reading all the images)\n\nquery_timestamp_ns = int(gaze_cpf[1].tracking_timestamp.total_seconds() * 1e9)\n\neye_gaze_info = get_nearest_eye_gaze(gaze_cpf, query_timestamp_ns)\n\nif eye_gaze_info:\n    # Re-project the eye gaze point onto the RGB camera data\n    vrs_file = "example.vrs"\n    vrs_data_provider = data_provider.create_vrs_data_provider(vrs_file)\n\n    rgb_stream_id = StreamId("214-1")\n    rgb_stream_label = vrs_data_provider.get_label_from_stream_id(rgb_stream_id)\n    device_calibration = vrs_data_provider.get_device_calibration()\n    rgb_camera_calibration = device_calibration.get_camera_calib(rgb_stream_label)\n\n    gaze_projection = get_gaze_vector_reprojection(\n    eye_gaze_info,\n    rgb_stream_label,\n    device_calibration,\n    rgb_camera_calibration,\n    depth_m,\n)\n    print(gaze_projection)\n'})})}),(0,n.jsx)(i.default,{value:"cpp",label:"C++",children:(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-cpp",children:'#include <EyeGazeReader.h>\nusing namespace projectaria::tools::mps;\n\nstd::string gazePath = "/path/to/mps/output/eye_gaze/eyegaze.csv";\nEyeGazes gazeCpf = readEyeGaze(gazePath);\n\n// Set default eye gaze depth for 3D points to 1 meter\nfloat depthM = 1.0f;\nEigen::Vector3d gazePointCpf = getEyeGazePointAtDepth(gazeCpf[0].yaw, gazeCpf[0].pitch, depthM);\n'})})})]}),"\n",(0,n.jsx)(t.h3,{id:"yawpitch-to-3d-vector-conversion",children:"Yaw/Pitch to 3D vector conversion\u200b"}),"\n",(0,n.jsxs)(t.p,{children:["Convert the gaze angles into 3D vectors. To convert a gaze measurement (yaw/pitch) into a 3D gaze vector originating at the origin of CPF (or 3D gaze point) use the ",(0,n.jsx)(t.code,{children:"Eigen::Vector3d getEyeGazePointAtDepth"})," operation in ",(0,n.jsx)(t.a,{href:"https://github.com/facebookresearch/projectaria_tools/blob/main/core/mps/EyeGazeReader.h#L40",children:"EyeGazeReader.h"}),". This function can be used with the new model output and old model output (go to ",(0,n.jsx)(t.a,{href:"/projectaria_tools/docs/data_formats/mps/mps_eye_gaze",children:"Eye Gaze Data Format"})," for more information about Eye Gaze changes)."]}),"\n",(0,n.jsxs)(t.h3,{id:"get-central-pupil-framecpf-to-device-transforms-python",children:["Get ",(0,n.jsx)(t.a,{href:"/projectaria_tools/docs/data_formats/coordinate_convention/3d_coordinate_frame_convention#the-nominal-central-pupil-frame-cpf",children:"Central Pupil Frame(CPF)"})," to Device Transforms (Python)"]}),"\n",(0,n.jsx)("div",{id:"helper"}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-python",children:'vrs_file = "example.vrs"\nvrs_data_provider = data_provider.create_vrs_data_provider(vrs_file)\nrgb_stream_id = StreamId("214-1")\nrgb_stream_label = vrs_data_provider.get_label_from_stream_id(rgb_stream_id)\ndevice_calibration = vrs_data_provider.get_device_calibration()\nrgb_camera_calibration = device_calibration.get_camera_calib(rgb_stream_label)\n# transform_cpf_sensor is the transform of sensor to cpf frame\ntransform_cpf_sensor = device_calibration.get_transform_cpf_sensor(stream_id_label)\n'})}),"\n",(0,n.jsx)(t.h2,{id:"helper-functions-to-support-the-new-eye-gaze-model",children:"Helper functions to support the new Eye Gaze model"}),"\n",(0,n.jsxs)(t.p,{children:["The following commands can be used to generate ",(0,n.jsx)(t.code,{children:"yaw_rads_cpf"})," values."]}),"\n",(0,n.jsx)(t.h3,{id:"get-combined-gaze-direction-and-depth-from-left-and-right-gaze-direction-angles",children:"Get combined gaze direction and depth from left and right gaze direction angles"}),"\n",(0,n.jsxs)(o.default,{groupId:"programming-language",children:[(0,n.jsx)(i.default,{value:"python",label:"Python",children:(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-python",children:'gaze_path = "/path/to/mps/output/eye_gaze/general_eye_gaze.csv"\ngaze_cpf = mps.read_eyegaze(eye_gaze_path)\ndepth, combined_yaw, combined_pitch = (\n    mps.compute_depth_and_combined_gaze_direction(\n        gaze_cpf[1].vergence.left_yaw, gaze_cpf[1].vergence.right_yaw, gaze_cpf[1].pitch\n    )\n)\n'})})}),(0,n.jsx)(i.default,{value:"cpp",label:"C++",children:(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-cpp",children:"float depthM = NAN, combinedYawRads = NAN, combinedPitchRads = NAN;\n std::tie(depthM, combinedYawRads, combinedPitchRads) =\n     computeDepthAndCombinedGazeDirection(eyegazeValues[0].\nleft_yaw, eyegazeValues[0].right_yaw, eyegazeValues[0].pitch);\n"})})})]}),"\n",(0,n.jsx)(t.h3,{id:"get-gaze-intersection-in-3d-coordinates-in-cpf-frame-from-left-and-right-gaze-direction-angles",children:"Get gaze intersection in 3D coordinates in CPF frame from left and right gaze direction angles"}),"\n",(0,n.jsxs)(o.default,{groupId:"programming-language",children:[(0,n.jsx)(i.default,{value:"python",label:"Python",children:(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-python",children:"gaze_x, gaze_y, gaze_z = mps.get_gaze_intersection_point(\n    gaze_cpf[1].vergence.left_yaw,\n    gaze_cpf[1].vergence.right_yaw,\n    gaze_cpf[1].pitch\n)\n"})})}),(0,n.jsx)(i.default,{value:"cpp",label:"C++",children:(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-cpp",children:"const Eigen::Vector3d gazePoint = getGazeIntersectionPoint(eyegazeValues[0].\nleft_yaw, eyegazeValues[0].right_yaw, eyegazeValues[0].pitch);\n"})})})]}),"\n",(0,n.jsx)(t.h3,{id:"get-left-and-right-gaze-directions-in-cpf-frame",children:"Get left and right gaze directions in CPF frame"}),"\n",(0,n.jsx)(t.p,{children:"Get left_gaze and right_gaze direction vectors (numpy arrays) given left and right yaws and common pitch. The vectors are from left and right eye positions to the point of intersection. Currently left eye and right eye origins are [0.0315, 0, 0] and [-0.0315, 0, 0] in CPF coordinates respectively."}),"\n",(0,n.jsxs)(o.default,{groupId:"programming-language",children:[(0,n.jsx)(i.default,{value:"python",label:"Python",children:(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-python",children:"left_gaze, right_gaze = mps.get_gaze_vectors(\n    gaze_cpf[1].vergence.left_yaw,\n    gaze_cpf[1].vergence.right_yaw,\n    gaze_cpf[1].pitch\n)\n"})})}),(0,n.jsx)(i.default,{value:"cpp",label:"C++",children:(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-cpp",children:"Eigen::Vector3d leftDirection, rightDirection;\n std::tie(leftDirection, rightDirection) = getGazeVectors(eyegazeValues[0].\nleft_yaw, eyegazeValues[0].right_yaw, eyegazeValues[0].pitch);\n"})})})]})]})}function h(e={}){const{wrapper:t}={...(0,r.R)(),...e.components};return t?(0,n.jsx)(t,{...e,children:(0,n.jsx)(u,{...e})}):u(e)}},7227:(e,t,a)=>{a.r(t),a.d(t,{default:()=>i});a(96540);var n=a(34164);const r={tabItem:"tabItem_Ymn6"};var o=a(74848);function i(e){let{children:t,hidden:a,className:i}=e;return(0,o.jsx)("div",{role:"tabpanel",className:(0,n.A)(r.tabItem,i),hidden:a,children:t})}},49489:(e,t,a)=>{a.r(t),a.d(t,{default:()=>j});var n=a(96540),r=a(34164),o=a(24245),i=a(56347),s=a(36494),l=a(62814),c=a(45167),d=a(69900);function p(e){return n.Children.toArray(e).filter((e=>"\n"!==e)).map((e=>{if(!e||(0,n.isValidElement)(e)&&function(e){const{props:t}=e;return!!t&&"object"==typeof t&&"value"in t}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}function u(e){const{values:t,children:a}=e;return(0,n.useMemo)((()=>{const e=t??function(e){return p(e).map((e=>{let{props:{value:t,label:a,attributes:n,default:r}}=e;return{value:t,label:a,attributes:n,default:r}}))}(a);return function(e){const t=(0,c.XI)(e,((e,t)=>e.value===t.value));if(t.length>0)throw new Error(`Docusaurus error: Duplicate values "${t.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[t,a])}function h(e){let{value:t,tabValues:a}=e;return a.some((e=>e.value===t))}function g(e){let{queryString:t=!1,groupId:a}=e;const r=(0,i.W6)(),o=function(e){let{queryString:t=!1,groupId:a}=e;if("string"==typeof t)return t;if(!1===t)return null;if(!0===t&&!a)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return a??null}({queryString:t,groupId:a});return[(0,l.aZ)(o),(0,n.useCallback)((e=>{if(!o)return;const t=new URLSearchParams(r.location.search);t.set(o,e),r.replace({...r.location,search:t.toString()})}),[o,r])]}function _(e){const{defaultValue:t,queryString:a=!1,groupId:r}=e,o=u(e),[i,l]=(0,n.useState)((()=>function(e){let{defaultValue:t,tabValues:a}=e;if(0===a.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(t){if(!h({value:t,tabValues:a}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${t}" but none of its children has the corresponding value. Available values are: ${a.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return t}const n=a.find((e=>e.default))??a[0];if(!n)throw new Error("Unexpected error: 0 tabValues");return n.value}({defaultValue:t,tabValues:o}))),[c,p]=g({queryString:a,groupId:r}),[_,m]=function(e){let{groupId:t}=e;const a=function(e){return e?`docusaurus.tab.${e}`:null}(t),[r,o]=(0,d.Dv)(a);return[r,(0,n.useCallback)((e=>{a&&o.set(e)}),[a,o])]}({groupId:r}),f=(()=>{const e=c??_;return h({value:e,tabValues:o})?e:null})();(0,s.A)((()=>{f&&l(f)}),[f]);return{selectedValue:i,selectValue:(0,n.useCallback)((e=>{if(!h({value:e,tabValues:o}))throw new Error(`Can't select invalid tab value=${e}`);l(e),p(e),m(e)}),[p,m,o]),tabValues:o}}var m=a(11062);const f={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var v=a(74848);function y(e){let{className:t,block:a,selectedValue:n,selectValue:i,tabValues:s}=e;const l=[],{blockElementScrollPositionUntilNextRender:c}=(0,o.a_)(),d=e=>{const t=e.currentTarget,a=l.indexOf(t),r=s[a].value;r!==n&&(c(t),i(r))},p=e=>{let t=null;switch(e.key){case"Enter":d(e);break;case"ArrowRight":{const a=l.indexOf(e.currentTarget)+1;t=l[a]??l[0];break}case"ArrowLeft":{const a=l.indexOf(e.currentTarget)-1;t=l[a]??l[l.length-1];break}}t?.focus()};return(0,v.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,r.A)("tabs",{"tabs--block":a},t),children:s.map((e=>{let{value:t,label:a,attributes:o}=e;return(0,v.jsx)("li",{role:"tab",tabIndex:n===t?0:-1,"aria-selected":n===t,ref:e=>l.push(e),onKeyDown:p,onClick:d,...o,className:(0,r.A)("tabs__item",f.tabItem,o?.className,{"tabs__item--active":n===t}),children:a??t},t)}))})}function b(e){let{lazy:t,children:a,selectedValue:o}=e;const i=(Array.isArray(a)?a:[a]).filter(Boolean);if(t){const e=i.find((e=>e.props.value===o));return e?(0,n.cloneElement)(e,{className:(0,r.A)("margin-top--md",e.props.className)}):null}return(0,v.jsx)("div",{className:"margin-top--md",children:i.map(((e,t)=>(0,n.cloneElement)(e,{key:t,hidden:e.props.value!==o})))})}function z(e){const t=_(e);return(0,v.jsxs)("div",{className:(0,r.A)("tabs-container",f.tabList),children:[(0,v.jsx)(y,{...t,...e}),(0,v.jsx)(b,{...t,...e})]})}function j(e){const t=(0,m.default)();return(0,v.jsx)(z,{...e,children:p(e.children)},String(t))}},28453:(e,t,a)=>{a.d(t,{R:()=>i,x:()=>s});var n=a(96540);const r={},o=n.createContext(r);function i(e){const t=n.useContext(o);return n.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function s(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:i(e.components),n.createElement(o.Provider,{value:t},e.children)}}}]);